# Stylometric Approaches to Authorship Attribution: Distinguishing Human-Written and ChatGPT-4o-mini Generated Texts
This repository contains the code, data, and results for a stylometric study on authorship attribution.  
The project investigates whether function-word-based stylometry can reliably distinguish human-written essays from ChatGPT-4o-mini generated texts. We evaluate three classifiers and examine how essay representation, text length, training data size, and topic relevance affect classification performance.  


### Authors
- Jingqi He  
  Email: [j.he-40@sms.ed.ac.uk](mailto:j.he-40@sms.ed.ac.uk)  
- Rongzhi Chen  
  Email: [r.chen-52@sms.ed.ac.uk](mailto:r.chen-52@sms.ed.ac.uk)  
- Shizhao Xiong  
  Email: [s.xiong-8@sms.ed.ac.uk](mailto:s.xiong-8@sms.ed.ac.uk)  

### Supervisor:
- Gordon J. Ross, Reader at the University of Edinburgh  
  Email: [gordon.ross@ed.ac.uk](mailto:gordon.ross@ed.ac.uk)

---

### Project Overview  

This study addresses four key research questions in authorship attribution, each implemented as a separate study with corresponding scripts and results:  

1. **Study 1 – Aggregated vs. Individual Treatment of Human Essays**  
   Whether human essays should be treated as a single aggregated class or as distinct authorial voices.  

2. **Study 2 – Impact of Essay Length**  
   How essay length, including 200-word excerpts, influences classifier performance.  

3. **Study 3 – Impact of Training Data Size**  
   How performance shifts when training data is reduced (100%, 50%, 20%, 10%).  

4. **Study 4 – Influence of Topic Relevance**  
   Whether classifiers generalise across topics or rely on topic-specific features.  

---

### Repository Structure

- **Code**  
  R scripts for data preprocessing, feature extraction, and classifier training.
  - `setup.R` — Environment setup and helper functions
  - **study1** — Scripts for testing essay representation strategies  
    - `howessayrepresentation.R`  
  - **study2** — Scripts for parameter selection and excerpt-based analysis  
    - `essaylength.R`  
  - **study3** — Scripts for training data size reduction  
    - `traindatasize.R`  
  - **study4** — Scripts for topic relevance experiments  
    - `all_other_topics.R`, `only_one_other_topic.R`, `only_this_topic.R`  

- **data**  
  - **rawtext**  
    - `GPTessays.zip`: Full-length essays generated by ChatGPT-4o-mini  
    - `humanessays.zip`: Full-length human-authored essays  

  - **functionwords**  
    - `GPTfunctionwords.zip`: Function word counts for full GPT essays  
    - `humanfunctionwords.zip`: Function word counts for full human essays  
    - `70functionword.txt`: List of the 70 function words used in the analysis  
    - `titles.zip`: Essay titles (included for completeness, not required for analysis)  

  - **excerptsfunctionwords**  
    - `GPTexcerpts.zip`: Function word counts from the **first 200 words** of each GPT essay  
    - `humanexcerpts.zip`: Function word counts from the **first 200 words** of each human essay  
    - *(used in **Study 2**, where we test the impact of shorter excerpts on classifier performance)*


- **Results**  
  Organised outputs from each of the four studies.
  - `MDS_plot.png` — Multi-Dimensional Scaling plot illustrating essay similarity
  - **study1** — Classifier results for essay representation (Delta, RF, SVMs, in `.png`)  
  - **study2** — Results for parameter tuning and excerpt-based analysis (Delta, RF, SVMs, in `.png`)  
  - **study3** — Results for training data size reduction (Delta, RF, SVMs, in `.png`)  
  - **study4** — Results for topic relevance experiments  
    - **figures** — Performance plots and cross-scenario comparisons (`.png`)  
    - **tables** — Summary tables of classifier performance (`.xlsx`)  

---

### Classifiers Evaluated  

- **Burrows’ Delta**
- **Random Forest**  
- **Support Vector Machines**  

Performance was assessed using accuracy, precision, recall, and F1 score.  

---

### How to Use  

1. **Prepare Data**  
   - Extract `GPTessays.zip`, `humanessays.zip`, and `functionwords.zip`.  

2. **Run Analyses**  
   - Use the R scripts in the `Code/` folder.  
   - Example usage:  
     - `form_of_data.R` → Study 1  
     - `WordLength.R` → Study 2  
     - `traindata_size.R` → Study 3  
     - `Except topic.R`, `Except_only_topic.R` and `OnlyTopic.R` → Study 4  

---

### Future Directions  

- Extend the dataset to include texts from other large language models.  
- Investigate hybrid authorship (human + AI collaboration).  
- Explore ensemble methods to improve robustness across topics and text lengths.  

---

### Additional Information  

For inquiries or further details, please contact the research team or the supervisor.  
